{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import urlparse # python 2.7\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import time\n",
    "\n",
    "class TwitterNewAPI(object):\n",
    "    \"\"\"\n",
    "    TwitterAPI class allows the Connection to Twitter via OAuth\n",
    "    once you have registered with Twitter and receive the \n",
    "    necessary credentials \n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        consumer_key = 'iZeky8F6lzswX8IiRrTxSfM1x'\n",
    "        consumer_secret = 'uEUEpov0JekhVwFIpmdl4uo3MvwkyKsBJTu9heqzJtuZONlTbM'\n",
    "        access_token = '719064415862005760-e4rKMdRa0NNYP9MuYb5YfyDaHpXgS4y'\n",
    "        access_secret = 'BukMZTpCYo2FyYnDdC2vdfMmyH1XTwDf9Wt39Dha6ZHyA'\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_secret = access_secret\n",
    "        self.retries = 3\n",
    "        self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)\n",
    "        self.api = twitter.Twitter(auth=self.auth)\n",
    "        \n",
    "        # logger initialisation\n",
    "        appName = 'whotestcsv'\n",
    "        self.logger = logging.getLogger(appName)\n",
    "        #self.logger.setLevel(logging.DEBUG)\n",
    "        # create console handler and set level to debug\n",
    "        logPath = '/home/ubuntu/anaconda2'\n",
    "        fileName = appName\n",
    "        fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(logPath, fileName))\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        fileHandler.setFormatter(formatter)\n",
    "        self.logger.addHandler(fileHandler) \n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        \n",
    "        # Save to JSON file initialisation\n",
    "        #jsonFpath = '/home/ubuntu/anaconda2'\n",
    "        #jsonFname = 'M10307916'\n",
    "        #self.jsonSaver = IO_json(jsonFpath, jsonFname)\n",
    "        \n",
    "        # Save to CSV file initialisation\n",
    "        csvFpath = '/home/ubuntu/anaconda2'\n",
    "        csvFname = 'M10307916'        \n",
    "        self.csvSaver = IO_csv(csvFpath, csvFname)\n",
    "      \n",
    "    def searchTwitter(self, q, max_res=10,**kwargs):\n",
    "        search_results = self.api.search.tweets(q=q, count=10,lang='en',locale='en', **kwargs)\n",
    "        statuses = search_results['statuses']\n",
    "        #screen_name = statuses['screen_name']\n",
    "        #screen_names = self.api.statuses.user_timeline(screen_name=screen_name)\n",
    "        # max_results = min(1000, max_res) \n",
    "        #max_results = min(100000, 100000) \n",
    "        max_results =1000 \n",
    "        for _ in range(100):\n",
    "            try:\n",
    "                next_results = search_results['search_metadata']['next_results']\n",
    "                # self.logger.info('info in searchTwitter - next_results:%s'% next_results[1:])\n",
    "            except KeyError as e:\n",
    "            \t#self.logger.error('error in searchTwitter: %s', %(e))\n",
    "                break\n",
    "            \n",
    "            next_results = urlparse.parse_qsl(next_results[1:]) # python 2.7\n",
    "            #next_results = urllib.parse.parse_qsl(next_results[1:])\n",
    "            # self.logger.info('info in searchTwitter - next_results[max_id]:', next_results[0:])\n",
    "            kwargs = dict(next_results)\n",
    "            # self.logger.info('info in searchTwitter - next_results[max_id]:%s'% kwargs['max_id'])\n",
    "            search_results = self.api.search.tweets(**kwargs)\n",
    "            statuses += search_results['statuses']\n",
    "            self.saveTweets(search_results['statuses'])\n",
    "            \n",
    "            if len(statuses) > max_results:\n",
    "                self.logger.info('info in searchTwitter - got %i tweets - max: %i' %(len(statuses), max_results))\n",
    "                break\n",
    "        return statuses\n",
    "\n",
    "    def saveTweets(self, statuses):\n",
    "        # Saving to JSON File\n",
    "        # self.jsonSaver.save(statuses)\n",
    "    \n",
    "        #field ='id, id_str, created_at, name, profile_image_url, text, expanded_url, profile_image_url_https, time_zone'        \n",
    "        field =('id', 'created_at', 'user_id',  'user_name', 'text', 'expanded_url')\n",
    "        partofstatuses = []\n",
    "        ids = []\n",
    "        created_ats = []\n",
    "        user_ids = []\n",
    "        user_names = []\n",
    "        texts = []\n",
    "        expanded_urls = []\n",
    "        \n",
    "        for i in range(len(self.parseTweets(statuses))):\n",
    "            partofstatus = (self.parseTweets(statuses))[i]\n",
    "            print('partofstatus :',partofstatus)         \n",
    "            partofstatuses.append(partofstatus)    \n",
    "        \n",
    "        #print('partofstatuses:',partofstatuses[:])        \n",
    "        self.csvSaver.save(partofstatuses,'twitter',field)\n",
    "      \n",
    "    def parseTweets(self, statuses):\n",
    "        return [ (status['id'], \n",
    "                  status['created_at'].encode('utf-8'), \n",
    "                  status['user']['id'],\n",
    "                  status['user']['name'].encode('utf-8'), \n",
    "                  status['text'].encode('utf-8'), \n",
    "                  url['expanded_url']) \n",
    "                        for status in statuses \n",
    "                            for url in status['entities']['urls'] ]\n",
    "\n",
    "    def getTweets(self, q,  max_res=10):\n",
    "        \"\"\"\n",
    "        Make a Twitter API call whilst managing rate limit and errors.\n",
    "        \"\"\"\n",
    "        def handleError(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "\n",
    "            if wait_period > 3600: # Seconds\n",
    "                #self.logger.error('Too many retries in getTweets: %s', %(e))\n",
    "                self.logger.info('3600')\n",
    "                raise e\n",
    "            if e.e.code == 401:\n",
    "                #self.logger.error('error 401 * Not Authorised * in getTweets: %s', %(e))\n",
    "                return None\n",
    "            elif e.e.code == 404:\n",
    "                #self.logger.error('error 404 * Not Found * in getTweets: %s', %(e))\n",
    "                return None\n",
    "            elif e.e.code == 429: \n",
    "                #self.logger.error('error 429 * API Rate Limit Exceeded * in getTweets: %s', %(e))\n",
    "                self.logger.info('429')\n",
    "                if sleep_when_rate_limited:\n",
    "                    #self.logger.error('error 429 * Retrying in 15 minutes * in getTweets: %s', %(e))\n",
    "                    sys.stderr.flush()\n",
    "                    time.sleep(60*15 + 5)\n",
    "                    #self.logger.info('error 429 * Retrying now * in getTweets: %s', %(e))\n",
    "                    return 2                    \n",
    "                else:\n",
    "                    raise e # Caller must handle the rate limiting issue\n",
    "            elif e.e.code in (500, 502, 503, 504):\n",
    "                self.logger.info('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period))\n",
    "                time.sleep(wait_period)\n",
    "                wait_period *= 1.5\n",
    "                return wait_period\n",
    "            else:\n",
    "                #self.logger.error('Exit - aborting - %s', %(e))\n",
    "                raise e\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                self.searchTwitter( q, max_res=10)\n",
    "                self.logger.info('Run')\n",
    "            except twitter.api.TwitterHTTPError as e:\n",
    "                self.logger.info('Exception')\n",
    "                error_count = 0\n",
    "                wait_period  = 2\n",
    "                wait_period = handleError(e, wait_period)               \n",
    "                if wait_period is None:\n",
    "                    self.logger.info('Return')\n",
    "                    return\n",
    "\n",
    "class IO_csv(object):\n",
    "    def __init__(self, filepath, filename, filesuffix='csv'):\n",
    "        self.filepath = filepath       # /path/to/file  without the '/' at the end\n",
    "        self.filename = filename       # FILE_NAME\n",
    "        self.filesuffix = filesuffix\n",
    "        # self.file_io = os.path.join(dir_name, '.'.join((base_filename, filename_suffix)))\n",
    "\n",
    "    def save(self, data, NTname, fields):\n",
    "        # NTname = Name of the NamedTuple\n",
    "        # fields = header of CSV - list of the fields name\n",
    "        NTuple = namedtuple(NTname, fields)\n",
    "        \n",
    "        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n",
    "            # Append existing file\n",
    "            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'ab') as f:\n",
    "                writer = csv.writer(f)\n",
    "                # writer.writerow(fields) # fields = header of CSV\n",
    "                writer.writerows([row for row in map(NTuple._make, data)])\n",
    "                # list comprehension using map on the NamedTuple._make() iterable and the data file to be saved\n",
    "                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file\n",
    "        else:\n",
    "            # Create new file\n",
    "            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'wb') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(fields) # fields = header of CSV - list of the fields name\n",
    "                writer.writerows([row for row in map(NTuple._make, data)])\n",
    "                #  list comprehension using map on the NamedTuple._make() iterable and the data file to be saved\n",
    "                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file\n",
    "            \n",
    "    def load(self, NTname, fields):\n",
    "        # NTname = Name of the NamedTuple\n",
    "        # fields = header of CSV - list of the fields name\n",
    "        NTuple = namedtuple(NTname, fields)\n",
    "        with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix),'rU') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in map(NTuple._make, reader):\n",
    "                # Using map on the NamedTuple._make() iterable and the reader file to be loaded\n",
    "                yield row\n",
    "\n",
    "\n",
    "class IO_json(object):\n",
    "    def __init__(self, filepath, filename, filesuffix='json'):\n",
    "        self.filepath = filepath        # /path/to/file  without the '/' at the end\n",
    "        self.filename = filename        # FILE_NAME\n",
    "        self.filesuffix = filesuffix\n",
    "        # self.file_io = os.path.join(dir_name, '.'.join((base_filename, filename_suffix)))\n",
    "\n",
    "    def save(self, data):\n",
    "        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):\n",
    "            # Append existing file\n",
    "            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'a', encoding='utf-8') as f:\n",
    "                f.write(unicode(json.dumps(data, ensure_ascii= False))) # In python 3, there is no \"unicode\" function \n",
    "                # f.write(json.dumps(data, ensure_ascii= False)) # create a \\\" escape char for \" in the saved file        \n",
    "        else:\n",
    "            # Create new file\n",
    "            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'w', encoding='utf-8') as f:\n",
    "                f.write(unicode(json.dumps(data, ensure_ascii= False)))\n",
    "                # f.write(json.dumps(data, ensure_ascii= False))    \n",
    "\n",
    "    def load(self):\n",
    "        with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "     \n",
    "t=TwitterNewAPI()\n",
    "q=\"nba\"\n",
    "#tsearch=t.searchTwitter( q, max_res=10)\n",
    "tsearch=t.getTweets( q, max_res=10)\n",
    "#print(t.parseTweets(tsearch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
